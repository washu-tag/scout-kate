FROM quay.io/jupyter/pyspark-notebook:spark-3.5.3

# SECURITY SCAN TEST â€” REVERT THIS COMMENT TO UNDO THE IMAGE REBUILD TRIGGER

# Install uv for fast package installation
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

COPY --chown=${NB_UID}:${NB_GID} requirements.txt /tmp/

# Copy spark jars, be sure to download the same jars in the pyspark notebook image and the temporal python worker image
ADD --chown=${NB_UID}:${NB_GID} \
    https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.3.0/delta-spark_2.12-3.3.0.jar \
    https://repo1.maven.org/maven2/io/delta/delta-storage/3.3.0/delta-storage-3.3.0.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.2/hadoop-common-3.2.2.jar \
    https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs/3.2.2/hadoop-hdfs-3.2.2.jar \
    https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar \
    https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.793/aws-java-sdk-bundle-1.12.793.jar \
    https://github.com/washu-tag/smolder/releases/download/0.1.0-20250306/smolder_2.12-0.1.0-SNAPSHOT.jar \
    ${SPARK_HOME}/jars/

RUN uv pip install --system --no-cache -r /tmp/requirements.txt && \
    fix-permissions "${CONDA_DIR}" && \
    fix-permissions "/home/${NB_USER}" && \
    rm /tmp/requirements.txt

# Install nb_conda_kernels (auto-discovers conda envs as kernels), faiss-gpu, and extra Python versions
RUN mamba install -y -n base nb_conda_kernels faiss-gpu && \
    mamba clean -afy

# Configure conda to store user environments in home directory (persisted via PVC)
RUN printf "envs_dirs:\n  - /home/jovyan/envs\n  - /opt/conda/envs\n" >> /opt/conda/.condarc

ADD --chown=${NB_UID}:${NB_GID} samples /opt/scout/samples
